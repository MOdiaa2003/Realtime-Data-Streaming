
version: "3.8"

services:

  postgres:
    image: postgres:17
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-webserver:
    image: apache/airflow:2.10.5
    container_name: airflow_webserver
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: "cYcnRjM4KFn93cV9pU7Wzcd5t82gsoDE6sGNBEPMUDo="
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
      AIRFLOW__WEBSERVER__WORKERS: 2
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/tmp
    ports:
      - "8080:8080"
    command: >
     bash -c "airflow db upgrade && 
              airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true && 
              airflow webserver"
    restart: always

  airflow-scheduler:
    image: apache/airflow:2.10.5
    container_name: airflow_scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: "cYcnRjM4KFn93cV9pU7Wzcd5t82gsoDE6sGNBEPMUDo="
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/tmp
    command: airflow scheduler
    restart: always

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper2
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: always
  
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka_job1
    ports:
      - "9092:9092"   # external access (Windows host)
      - "29092:29092" # internal access (Spark containers)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper2:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka_job1:29092,OUTSIDE://localhost:9092
      KAFKA_LISTENERS: INSIDE://0.0.0.0:29092,OUTSIDE://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
 
  schema-registry:
    image: confluentinc/cp-schema-registry:7.4.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka_job1:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8083
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
  
  control-center:
    image: confluentinc/cp-enterprise-control-center:7.4.0
    hostname: control-center
    container_name: control-center
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'kafka_job1:29092'
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8083"
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      CONFLUENT_METRICS_ENABLE: 'false'
      PORT: 9021
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9021/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: always

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master2
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_DIRS=/tmp
      - SPARK_MASTER_HOST=spark-master2
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "8082:8080"   # Spark Master UI
      - "7077:7077"   # Spark Master port
    restart: always

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master2:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
    depends_on:
      - spark-master
    ports:
      - "8081:8081"   # Spark Worker UI
    restart: always
  cassandra:
      image: cassandra:4.1
      container_name: cassandra
      environment:
        CASSANDRA_CLUSTER_NAME: "TestCluster"
        CASSANDRA_DC: "DC1"
        CASSANDRA_RACK: "Rack1"
        CASSANDRA_ENDPOINT_SNITCH: "GossipingPropertyFileSnitch"
      ports:
        - "9042:9042"   # CQL port
        - "9160:9160"   # Thrift port (optional, legacy)
      volumes:
        - cassandra-data:/var/lib/cassandra
      healthcheck:
        test: ["CMD-SHELL", "cqlsh -e 'describe cluster' || exit 1"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 60s
      restart: always  
  

volumes:
  postgres-db-volume:
  cassandra-data: